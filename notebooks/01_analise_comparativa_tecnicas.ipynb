{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AnÃ¡lise Comparativa de TÃ©cnicas de PÃ³s-Processamento para Data Linkage\n",
    "## SIM Ã— SINAN-TB\n",
    "\n",
    "Este notebook avalia mÃºltiplas tÃ©cnicas de Machine Learning para identificar pares adicionais no data linkage entre as bases SIM (Sistema de InformaÃ§Ã£o sobre Mortalidade) e SINAN-TB (Sistema de InformaÃ§Ã£o de Agravos de NotificaÃ§Ã£o - Tuberculose).\n",
    "\n",
    "### TÃ©cnicas Avaliadas:\n",
    "1. **ClÃ¡ssicas:** RegressÃ£o LogÃ­stica, SVM\n",
    "2. **Ensemble:** Random Forest, Gradient Boosting (XGBoost, LightGBM)\n",
    "3. **Redes Neurais:** MLP (Multi-Layer Perceptron)\n",
    "4. **Balanceamento:** SMOTE, Class Weights\n",
    "5. **Threshold Optimization:** OtimizaÃ§Ã£o de ponto de corte\n",
    "6. **Stacking:** CombinaÃ§Ã£o de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# InstalaÃ§Ã£o de dependÃªncias (executar apenas uma vez)\n",
    "!pip install -q pandas numpy scikit-learn xgboost lightgbm imbalanced-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, \n",
    "    precision_recall_curve, roc_curve, f1_score, precision_score, \n",
    "    recall_score, average_precision_score\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ConfiguraÃ§Ãµes de visualizaÃ§Ã£o\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"Bibliotecas carregadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregamento e PreparaÃ§Ã£o dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dados\n",
    "from pathlib import Path\n",
    "\n",
    "def find_data_path(filename: str = 'COMPARADORSEMIDENT.csv') -> Path:\n",
    "    for base in (Path.cwd(), *Path.cwd().parents):\n",
    "        candidate = base / 'data' / filename\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "    raise FileNotFoundError(\n",
    "        f\"Nao foi possivel localizar data/{filename}. CWD={Path.cwd()}\"\n",
    "    )\n",
    "\n",
    "DATA_PATH = find_data_path()\n",
    "df = pd.read_csv(DATA_PATH, sep=';', low_memory=False)\n",
    "print(f\"Lendo arquivo: {DATA_PATH.resolve()}\")\n",
    "\n",
    "# Limpar nomes das colunas\n",
    "def clean_col(col):\n",
    "    return col.split(',')[0] if ',' in col else col\n",
    "\n",
    "df.columns = [clean_col(c) for c in df.columns]\n",
    "\n",
    "# Converter todas as colunas numÃ©ricas\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == object:\n",
    "        try:\n",
    "            df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', '.'), errors='coerce')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "print(f\"Shape do dataset: {df.shape}\")\n",
    "print(f\"\\nDistribuiÃ§Ã£o da variÃ¡vel PAR:\")\n",
    "print(df['PAR'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar variÃ¡vel target binÃ¡ria\n",
    "df['TARGET'] = (df['PAR'].isin([1, 2])).astype(int)\n",
    "\n",
    "# Definir features para o modelo\n",
    "FEATURE_COLS = [\n",
    "    # Scores de NOME\n",
    "    'NOME prim frag igual',\n",
    "    'NOME ult frag igual', \n",
    "    'NOME qtd frag iguais',\n",
    "    'NOME qtd frag raros',\n",
    "    'NOME qtd frag comuns',\n",
    "    'NOME qtd frag muito parec',\n",
    "    'NOME qtd frag abrev',\n",
    "    # Scores de NOMEMAE\n",
    "    'NOMEMAE prim frag igual',\n",
    "    'NOMEMAE ult frag igual',\n",
    "    'NOMEMAE qtd frag iguais',\n",
    "    'NOMEMAE qtd frag raros',\n",
    "    'NOMEMAE qtd frag comuns',\n",
    "    'NOMEMAE qtd frag muito parec',\n",
    "    'NOMEMAE qtd frag abrev',\n",
    "    # Scores de DTNASC\n",
    "    'DTNASC dt iguais',\n",
    "    'DTNASC dt ap 1digi',\n",
    "    'DTNASC dt inv dia',\n",
    "    'DTNASC dt inv mes',\n",
    "    'DTNASC dt inv ano',\n",
    "    # Scores de localizaÃ§Ã£o\n",
    "    'CODMUNRES uf igual',\n",
    "    'CODMUNRES local igual',\n",
    "    'CODMUNRES local prox',\n",
    "    # Scores de endereÃ§o\n",
    "    'ENDERECO via igual',\n",
    "    'ENDERECO via prox',\n",
    "    'ENDERECO numero igual',\n",
    "    'ENDERECO compl prox',\n",
    "    'ENDERECO texto prox',\n",
    "    'ENDERECO tokens jacc',\n",
    "    # Nota final do OpenRecLink\n",
    "    'nota final'\n",
    "]\n",
    "\n",
    "# Verificar quais colunas existem\n",
    "available_features = [col for col in FEATURE_COLS if col in df.columns]\n",
    "print(f\"Features disponÃ­veis: {len(available_features)} de {len(FEATURE_COLS)}\")\n",
    "\n",
    "# Criar matriz de features\n",
    "X = df[available_features].copy()\n",
    "y = df['TARGET'].copy()\n",
    "\n",
    "# Tratar valores ausentes\n",
    "X = X.fillna(0)\n",
    "\n",
    "print(f\"\\nShape de X: {X.shape}\")\n",
    "print(f\"DistribuiÃ§Ã£o de y: {y.value_counts().to_dict()}\")\n",
    "print(f\"Taxa de desbalanceamento: 1:{int(y.value_counts()[0]/y.value_counts()[1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering adicional\n",
    "\n",
    "# 1. CombinaÃ§Ãµes de features\n",
    "X['nome_score_total'] = X['NOME prim frag igual'] + X['NOME ult frag igual'] + X['NOME qtd frag iguais']\n",
    "X['mae_score_total'] = X['NOMEMAE prim frag igual'] + X['NOMEMAE ult frag igual'] + X['NOMEMAE qtd frag iguais']\n",
    "X['dtnasc_score_total'] = X['DTNASC dt iguais'] + X['DTNASC dt ap 1digi'] + X['DTNASC dt inv dia'] + X['DTNASC dt inv mes']\n",
    "X['endereco_score_total'] = X['ENDERECO via igual'] + X['ENDERECO via prox'] + X['ENDERECO texto prox']\n",
    "\n",
    "# 2. InteraÃ§Ãµes\n",
    "X['nome_x_dtnasc'] = X['NOME qtd frag iguais'] * X['DTNASC dt iguais']\n",
    "X['nome_x_mae'] = X['NOME qtd frag iguais'] * X['NOMEMAE qtd frag iguais']\n",
    "X['nome_x_endereco'] = X['NOME qtd frag iguais'] * X['endereco_score_total']\n",
    "\n",
    "# 3. Flags binÃ¡rios\n",
    "X['nome_perfeito'] = (X['NOME qtd frag iguais'] >= 0.95).astype(int)\n",
    "X['dtnasc_perfeito'] = (X['DTNASC dt iguais'] == 1).astype(int)\n",
    "X['mae_presente'] = (X['NOMEMAE qtd frag iguais'] > 0).astype(int)\n",
    "X['endereco_match'] = (X['ENDERECO via igual'] > 0).astype(int)\n",
    "\n",
    "# 4. Adicionar informaÃ§Ã£o de situaÃ§Ã£o de encerramento (C_SITUENCE)\n",
    "X['obito_sinan'] = df['C_SITUENCE'].isin([3, 4]).astype(int)\n",
    "\n",
    "print(f\"Shape final de X apÃ³s feature engineering: {X.shape}\")\n",
    "print(f\"\\nNovas features criadas:\")\n",
    "new_features = [col for col in X.columns if col not in available_features]\n",
    "for f in new_features:\n",
    "    print(f\"  - {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DivisÃ£o dos Dados e PreparaÃ§Ã£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DivisÃ£o estratificada\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Treino: {X_train.shape[0]} amostras ({y_train.sum()} pares)\")\n",
    "print(f\"Teste: {X_test.shape[0]} amostras ({y_test.sum()} pares)\")\n",
    "\n",
    "# NormalizaÃ§Ã£o\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Guardar Ã­ndices para anÃ¡lise posterior\n",
    "test_indices = X_test.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DefiniÃ§Ã£o e Treinamento dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular peso das classes para lidar com desbalanceamento\n",
    "class_weight = {0: 1, 1: len(y_train[y_train==0]) / len(y_train[y_train==1])}\n",
    "print(f\"Peso das classes: {class_weight}\")\n",
    "\n",
    "# Definir modelos\n",
    "models = {\n",
    "    '1. Logistic Regression': LogisticRegression(\n",
    "        class_weight='balanced', max_iter=1000, random_state=42\n",
    "    ),\n",
    "    \n",
    "    '2. Random Forest': RandomForestClassifier(\n",
    "        n_estimators=200, max_depth=10, class_weight='balanced',\n",
    "        random_state=42, n_jobs=-1\n",
    "    ),\n",
    "    \n",
    "    '3. Gradient Boosting': GradientBoostingClassifier(\n",
    "        n_estimators=200, max_depth=5, learning_rate=0.1,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \n",
    "    '4. SVM (RBF)': SVC(\n",
    "        kernel='rbf', class_weight='balanced', probability=True,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \n",
    "    '5. MLP Neural Network': MLPClassifier(\n",
    "        hidden_layer_sizes=(128, 64, 32), activation='relu',\n",
    "        max_iter=500, random_state=42, early_stopping=True\n",
    "    )\n",
    "}\n",
    "\n",
    "print(f\"\\n{len(models)} modelos definidos para avaliaÃ§Ã£o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar e avaliar cada modelo\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Treinando: {name}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Usar dados escalados para SVM e MLP\n",
    "    if 'SVM' in name or 'MLP' in name:\n",
    "        X_tr, X_te = X_train_scaled, X_test_scaled\n",
    "    else:\n",
    "        X_tr, X_te = X_train.values, X_test.values\n",
    "    \n",
    "    # Treinar\n",
    "    model.fit(X_tr, y_train)\n",
    "    \n",
    "    # PrediÃ§Ãµes\n",
    "    y_pred = model.predict(X_te)\n",
    "    y_proba = model.predict_proba(X_te)[:, 1]\n",
    "    \n",
    "    # MÃ©tricas\n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'y_pred': y_pred,\n",
    "        'y_proba': y_proba,\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'auc_roc': roc_auc_score(y_test, y_proba),\n",
    "        'auc_pr': average_precision_score(y_test, y_proba)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nResultados:\")\n",
    "    print(f\"  Precision: {results[name]['precision']:.4f}\")\n",
    "    print(f\"  Recall:    {results[name]['recall']:.4f}\")\n",
    "    print(f\"  F1-Score:  {results[name]['f1']:.4f}\")\n",
    "    print(f\"  AUC-ROC:   {results[name]['auc_roc']:.4f}\")\n",
    "    print(f\"  AUC-PR:    {results[name]['auc_pr']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TÃ©cnicas com SMOTE (Oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar SMOTE para balanceamento\n",
    "print(\"Aplicando SMOTE...\")\n",
    "\n",
    "smote = SMOTE(random_state=42, k_neighbors=3)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"Antes do SMOTE: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"Depois do SMOTE: {pd.Series(y_train_smote).value_counts().to_dict()}\")\n",
    "\n",
    "# Treinar modelo com SMOTE\n",
    "rf_smote = RandomForestClassifier(\n",
    "    n_estimators=200, max_depth=10, random_state=42, n_jobs=-1\n",
    ")\n",
    "rf_smote.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "y_pred_smote = rf_smote.predict(X_test.values)\n",
    "y_proba_smote = rf_smote.predict_proba(X_test.values)[:, 1]\n",
    "\n",
    "results['6. Random Forest + SMOTE'] = {\n",
    "    'model': rf_smote,\n",
    "    'y_pred': y_pred_smote,\n",
    "    'y_proba': y_proba_smote,\n",
    "    'precision': precision_score(y_test, y_pred_smote),\n",
    "    'recall': recall_score(y_test, y_pred_smote),\n",
    "    'f1': f1_score(y_test, y_pred_smote),\n",
    "    'auc_roc': roc_auc_score(y_test, y_proba_smote),\n",
    "    'auc_pr': average_precision_score(y_test, y_proba_smote)\n",
    "}\n",
    "\n",
    "print(f\"\\nRandom Forest + SMOTE:\")\n",
    "print(f\"  Precision: {results['6. Random Forest + SMOTE']['precision']:.4f}\")\n",
    "print(f\"  Recall:    {results['6. Random Forest + SMOTE']['recall']:.4f}\")\n",
    "print(f\"  F1-Score:  {results['6. Random Forest + SMOTE']['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. OtimizaÃ§Ã£o de Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_threshold(y_true, y_proba, metric='f1'):\n",
    "    \"\"\"\n",
    "    Encontra o threshold Ã³timo para uma mÃ©trica especÃ­fica.\n",
    "    \"\"\"\n",
    "    thresholds = np.arange(0.01, 1.0, 0.01)\n",
    "    best_threshold = 0.5\n",
    "    best_score = 0\n",
    "    \n",
    "    results_th = []\n",
    "    \n",
    "    for th in thresholds:\n",
    "        y_pred_th = (y_proba >= th).astype(int)\n",
    "        \n",
    "        if y_pred_th.sum() == 0:  # Evitar divisÃ£o por zero\n",
    "            continue\n",
    "            \n",
    "        prec = precision_score(y_true, y_pred_th, zero_division=0)\n",
    "        rec = recall_score(y_true, y_pred_th)\n",
    "        f1 = f1_score(y_true, y_pred_th)\n",
    "        \n",
    "        results_th.append({\n",
    "            'threshold': th,\n",
    "            'precision': prec,\n",
    "            'recall': rec,\n",
    "            'f1': f1\n",
    "        })\n",
    "        \n",
    "        if metric == 'f1' and f1 > best_score:\n",
    "            best_score = f1\n",
    "            best_threshold = th\n",
    "        elif metric == 'recall' and rec > best_score and prec > 0.01:\n",
    "            best_score = rec\n",
    "            best_threshold = th\n",
    "        elif metric == 'precision' and prec > best_score and rec > 0.3:\n",
    "            best_score = prec\n",
    "            best_threshold = th\n",
    "    \n",
    "    return best_threshold, best_score, pd.DataFrame(results_th)\n",
    "\n",
    "# Otimizar threshold para o melhor modelo (Random Forest)\n",
    "best_model_name = '2. Random Forest'\n",
    "y_proba_best = results[best_model_name]['y_proba']\n",
    "\n",
    "# Threshold para mÃ¡ximo F1\n",
    "th_f1, score_f1, df_th = optimize_threshold(y_test, y_proba_best, 'f1')\n",
    "print(f\"Threshold Ã³timo para F1: {th_f1:.2f} (F1={score_f1:.4f})\")\n",
    "\n",
    "# Threshold para mÃ¡ximo Recall\n",
    "th_recall, score_recall, _ = optimize_threshold(y_test, y_proba_best, 'recall')\n",
    "print(f\"Threshold Ã³timo para Recall: {th_recall:.2f} (Recall={score_recall:.4f})\")\n",
    "\n",
    "# Threshold para mÃ¡xima Precision\n",
    "th_prec, score_prec, _ = optimize_threshold(y_test, y_proba_best, 'precision')\n",
    "print(f\"Threshold Ã³timo para Precision: {th_prec:.2f} (Precision={score_prec:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar curva de trade-off Precision-Recall por threshold\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Precision, Recall, F1 vs Threshold\n",
    "ax1 = axes[0]\n",
    "ax1.plot(df_th['threshold'], df_th['precision'], label='Precision', linewidth=2)\n",
    "ax1.plot(df_th['threshold'], df_th['recall'], label='Recall', linewidth=2)\n",
    "ax1.plot(df_th['threshold'], df_th['f1'], label='F1-Score', linewidth=2)\n",
    "ax1.axvline(x=th_f1, color='red', linestyle='--', alpha=0.7, label=f'Threshold Ã³timo (F1)')\n",
    "ax1.set_xlabel('Threshold', fontsize=12)\n",
    "ax1.set_ylabel('Score', fontsize=12)\n",
    "ax1.set_title('MÃ©tricas vs Threshold', fontsize=14)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Curva Precision-Recall\n",
    "ax2 = axes[1]\n",
    "precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_proba_best)\n",
    "ax2.plot(recall_curve, precision_curve, linewidth=2, color='blue')\n",
    "ax2.fill_between(recall_curve, precision_curve, alpha=0.2)\n",
    "ax2.set_xlabel('Recall', fontsize=12)\n",
    "ax2.set_ylabel('Precision', fontsize=12)\n",
    "ax2.set_title(f'Curva Precision-Recall (AUC-PR={results[best_model_name][\"auc_pr\"]:.3f})', fontsize=14)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('threshold_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ensemble: Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar ensemble com Stacking\n",
    "print(\"Construindo modelo Stacking...\")\n",
    "\n",
    "base_estimators = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, max_depth=8, class_weight='balanced', random_state=42)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=100, max_depth=4, random_state=42)),\n",
    "    ('lr', LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42))\n",
    "]\n",
    "\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=base_estimators,\n",
    "    final_estimator=LogisticRegression(class_weight='balanced'),\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "stacking_model.fit(X_train.values, y_train)\n",
    "\n",
    "y_pred_stack = stacking_model.predict(X_test.values)\n",
    "y_proba_stack = stacking_model.predict_proba(X_test.values)[:, 1]\n",
    "\n",
    "results['7. Stacking Ensemble'] = {\n",
    "    'model': stacking_model,\n",
    "    'y_pred': y_pred_stack,\n",
    "    'y_proba': y_proba_stack,\n",
    "    'precision': precision_score(y_test, y_pred_stack),\n",
    "    'recall': recall_score(y_test, y_pred_stack),\n",
    "    'f1': f1_score(y_test, y_pred_stack),\n",
    "    'auc_roc': roc_auc_score(y_test, y_proba_stack),\n",
    "    'auc_pr': average_precision_score(y_test, y_proba_stack)\n",
    "}\n",
    "\n",
    "print(f\"\\nStacking Ensemble:\")\n",
    "print(f\"  Precision: {results['7. Stacking Ensemble']['precision']:.4f}\")\n",
    "print(f\"  Recall:    {results['7. Stacking Ensemble']['recall']:.4f}\")\n",
    "print(f\"  F1-Score:  {results['7. Stacking Ensemble']['f1']:.4f}\")\n",
    "print(f\"  AUC-ROC:   {results['7. Stacking Ensemble']['auc_roc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ComparaÃ§Ã£o Final dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar DataFrame comparativo\n",
    "comparison_data = []\n",
    "for name, res in results.items():\n",
    "    comparison_data.append({\n",
    "        'Modelo': name,\n",
    "        'Precision': res['precision'],\n",
    "        'Recall': res['recall'],\n",
    "        'F1-Score': res['f1'],\n",
    "        'AUC-ROC': res['auc_roc'],\n",
    "        'AUC-PR': res['auc_pr']\n",
    "    })\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "df_comparison = df_comparison.sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARAÃ‡ÃƒO FINAL DOS MODELOS\")\n",
    "print(\"=\"*80)\n",
    "print(df_comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VisualizaÃ§Ã£o comparativa\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Barras de mÃ©tricas\n",
    "ax1 = axes[0]\n",
    "x = np.arange(len(df_comparison))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = ax1.bar(x - width, df_comparison['Precision'], width, label='Precision', color='#2ecc71')\n",
    "bars2 = ax1.bar(x, df_comparison['Recall'], width, label='Recall', color='#3498db')\n",
    "bars3 = ax1.bar(x + width, df_comparison['F1-Score'], width, label='F1-Score', color='#e74c3c')\n",
    "\n",
    "ax1.set_ylabel('Score', fontsize=12)\n",
    "ax1.set_title('ComparaÃ§Ã£o de MÃ©tricas por Modelo', fontsize=14)\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([m.split('. ')[1] if '. ' in m else m for m in df_comparison['Modelo']], \n",
    "                    rotation=45, ha='right')\n",
    "ax1.legend()\n",
    "ax1.set_ylim(0, 1.1)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 2: Curvas ROC\n",
    "ax2 = axes[1]\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(results)))\n",
    "\n",
    "for (name, res), color in zip(results.items(), colors):\n",
    "    fpr, tpr, _ = roc_curve(y_test, res['y_proba'])\n",
    "    ax2.plot(fpr, tpr, label=f\"{name.split('. ')[1] if '. ' in name else name} (AUC={res['auc_roc']:.3f})\",\n",
    "             color=color, linewidth=2)\n",
    "\n",
    "ax2.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "ax2.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax2.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax2.set_title('Curvas ROC', fontsize=14)\n",
    "ax2.legend(loc='lower right', fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. IdentificaÃ§Ã£o do Melhor Modelo para Recall vs Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar melhores modelos\n",
    "best_recall_model = df_comparison.loc[df_comparison['Recall'].idxmax(), 'Modelo']\n",
    "best_precision_model = df_comparison.loc[df_comparison['Precision'].idxmax(), 'Modelo']\n",
    "best_f1_model = df_comparison.loc[df_comparison['F1-Score'].idxmax(), 'Modelo']\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMENDAÃ‡Ã•ES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ MELHOR RECALL (encontrar mais pares):\")\n",
    "print(f\"   Modelo: {best_recall_model}\")\n",
    "print(f\"   Recall: {df_comparison[df_comparison['Modelo']==best_recall_model]['Recall'].values[0]:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ MELHOR PRECISION (menos falsos positivos):\")\n",
    "print(f\"   Modelo: {best_precision_model}\")\n",
    "print(f\"   Precision: {df_comparison[df_comparison['Modelo']==best_precision_model]['Precision'].values[0]:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ MELHOR F1 (equilÃ­brio):\")\n",
    "print(f\"   Modelo: {best_f1_model}\")\n",
    "print(f\"   F1-Score: {df_comparison[df_comparison['Modelo']==best_f1_model]['F1-Score'].values[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImportÃ¢ncia das features (Random Forest)\n",
    "rf_model = results['2. Random Forest']['model']\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Visualizar top 20\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance.head(20)\n",
    "plt.barh(range(len(top_features)), top_features['importance'], color='steelblue')\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('ImportÃ¢ncia', fontsize=12)\n",
    "plt.title('Top 20 Features mais Importantes (Random Forest)', fontsize=14)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 Features:\")\n",
    "print(feature_importance.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. AnÃ¡lise de Novos Pares Potenciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar o melhor modelo a todos os dados e identificar potenciais pares\n",
    "X_all = X.copy()\n",
    "y_proba_all = rf_model.predict_proba(X_all.values)[:, 1]\n",
    "\n",
    "df['ML_PROBA'] = y_proba_all\n",
    "df['ML_RANK'] = df['ML_PROBA'].rank(ascending=False)\n",
    "\n",
    "# Encontrar nÃ£o-pares com alta probabilidade (potenciais falsos negativos)\n",
    "falsos_negativos_potenciais = df[\n",
    "    (df['PAR'] == 0) & \n",
    "    (df['ML_PROBA'] >= 0.5)\n",
    "].sort_values('ML_PROBA', ascending=False)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"POTENCIAIS FALSOS NEGATIVOS (nÃ£o-pares com alta probabilidade ML)\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nTotal de candidatos com P(par) >= 0.5: {len(falsos_negativos_potenciais)}\")\n",
    "print(f\"\\nDistribuiÃ§Ã£o por passo:\")\n",
    "print(falsos_negativos_potenciais['PASSO'].value_counts().sort_index())\n",
    "\n",
    "print(f\"\\nDistribuiÃ§Ã£o por C_SITUENCE (encerramento):\")\n",
    "print(falsos_negativos_potenciais['C_SITUENCE'].value_counts().head())\n",
    "\n",
    "# Quantos tÃªm Ã³bito no SINAN?\n",
    "fn_com_obito = falsos_negativos_potenciais[falsos_negativos_potenciais['C_SITUENCE'].isin([3, 4])]\n",
    "print(f\"\\n>>> Com encerramento por Ã“BITO: {len(fn_com_obito)} ({len(fn_com_obito)/len(falsos_negativos_potenciais)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar lista de candidatos para revisÃ£o manual\n",
    "cols_export = ['PASSO', 'ML_PROBA', 'nota final', 'C_SITUENCE', \n",
    "               'R_DTNASC', 'C_DTNASC', 'R_BAIRES', 'C_BAIRES',\n",
    "               'R_IDLINHA', 'C_IDLINHA']\n",
    "\n",
    "candidatos_revisao = falsos_negativos_potenciais[cols_export].head(200)\n",
    "candidatos_revisao.to_csv('candidatos_revisao_ml.csv', index=False, sep=';')\n",
    "\n",
    "print(f\"\\nExportados {len(candidatos_revisao)} candidatos para revisÃ£o em 'candidatos_revisao_ml.csv'\")\n",
    "print(\"\\nPrimeiros 10 candidatos:\")\n",
    "print(candidatos_revisao.head(10).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Resumo Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESUMO FINAL DA ANÃLISE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "DATASET:\n",
    "  - Total de comparaÃ§Ãµes: {len(df):,}\n",
    "  - Pares verdadeiros: {df['TARGET'].sum()}\n",
    "  - Taxa de desbalanceamento: 1:{int((df['TARGET']==0).sum()/(df['TARGET']==1).sum())}\n",
    "\n",
    "MELHOR MODELO GERAL: {best_f1_model}\n",
    "  - F1-Score: {df_comparison[df_comparison['Modelo']==best_f1_model]['F1-Score'].values[0]:.4f}\n",
    "  - Recall: {df_comparison[df_comparison['Modelo']==best_f1_model]['Recall'].values[0]:.4f}\n",
    "  - Precision: {df_comparison[df_comparison['Modelo']==best_f1_model]['Precision'].values[0]:.4f}\n",
    "\n",
    "POTENCIAIS PARES ADICIONAIS IDENTIFICADOS:\n",
    "  - Com P(par) >= 0.5: {len(falsos_negativos_potenciais)}\n",
    "  - Com encerramento por Ã³bito: {len(fn_com_obito)}\n",
    "\n",
    "RECOMENDAÃ‡Ã•ES:\n",
    "  1. Para MAXIMIZAR RECALL: Usar threshold baixo (~0.15-0.20)\n",
    "  2. Para MAXIMIZAR PRECISION: Usar threshold alto (~0.70-0.80)\n",
    "  3. Priorizar revisÃ£o de candidatos com C_SITUENCE = 3 ou 4\n",
    "\"\"\")\n",
    "\n",
    "# Salvar resultados\n",
    "df_comparison.to_csv('comparacao_modelos.csv', index=False, sep=';')\n",
    "print(\"\\nResultados salvos em 'comparacao_modelos.csv'\")\n",
    "\n",
    "# Exportar metricas (CI-friendly)\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "results_dir = Path('results')\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "metrics_csv = Path('comparacao_modelos.csv')\n",
    "if metrics_csv.exists():\n",
    "    dfm = pd.read_csv(metrics_csv, sep=';')\n",
    "    # Espera colunas: Modelo, Precision, Recall, F1-Score, AUC-ROC, AUC-PR\n",
    "    best_f1 = dfm.loc[dfm['F1-Score'].idxmax()].to_dict()\n",
    "    best_precision = dfm.loc[dfm['Precision'].idxmax()].to_dict()\n",
    "    best_recall = dfm.loc[dfm['Recall'].idxmax()].to_dict()\n",
    "    payload = {\n",
    "        'notebook': '01_analise_comparativa_tecnicas',\n",
    "        'dataset': 'data/COMPARADORSEMIDENT.csv',\n",
    "        'best_by_f1': best_f1,\n",
    "        'best_by_precision': best_precision,\n",
    "        'best_by_recall': best_recall,\n",
    "    }\n",
    "else:\n",
    "    payload = {\n",
    "        'notebook': '01_analise_comparativa_tecnicas',\n",
    "        'dataset': 'data/COMPARADORSEMIDENT.csv',\n",
    "        'warning': 'comparacao_modelos.csv nao encontrado; execute o notebook por completo',\n",
    "    }\n",
    "\n",
    "out_path = results_dir / '01_analise_comparativa_tecnicas.results.json'\n",
    "out_path.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding='utf-8')\n",
    "print(f'Resultados exportados: {out_path.resolve()}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
